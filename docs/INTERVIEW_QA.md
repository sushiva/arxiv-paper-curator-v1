# Interview Q&A - arXiv Paper Curator RAG System

Complete question & answer guide for technical interviews about this project.

---

## ðŸ—ï¸ Architecture & Design

### Q: What is the overall architecture of your RAG system?

**A**: My RAG system uses a modern multi-database architecture with the following components:

```
User Query
    â†“
Streamlit Frontend (Python)
    â†“
FastAPI Backend
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚   OpenSearch    â”‚   Jina API   â”‚
â”‚ (Metadata)  â”‚ (Vector Search) â”‚ (Embeddings) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
OpenAI GPT-4o-mini (Answer Generation)
```

**Key Components**:
- **Frontend**: Streamlit (Python web framework)
- **Backend**: FastAPI (REST API)
- **Metadata DB**: PostgreSQL (paper metadata, authors, dates)
- **Vector DB**: OpenSearch (vector embeddings, semantic search)
- **Embeddings**: Jina AI API (text-to-vector conversion)
- **LLM**: OpenAI GPT-4o-mini (answer generation)
- **Deployment**: Railway.app (all services)

---

## ðŸ’¾ Database Design

### Q: Why do you need both PostgreSQL AND OpenSearch? Isn't that redundant?

**A**: No, they serve completely different purposes:

**PostgreSQL (Relational Database)**:
- **Purpose**: Store structured metadata
- **What it stores**: Paper titles, authors, publication dates, abstracts, categories
- **Why needed**:
  - Source of truth for paper metadata
  - ACID compliance (data integrity)
  - Complex relational queries
  - Joins, transactions, constraints

**Example PostgreSQL data**:
```json
{
  "id": 42,
  "arxiv_id": "2511.18405v1",
  "title": "Monet: Reasoning in Latent Visual Space",
  "authors": ["John Doe", "Jane Smith"],
  "published_date": "2024-11-27"
}
```

**OpenSearch (Vector Database)**:
- **Purpose**: Semantic search using vector embeddings
- **What it stores**: Text chunks, vector embeddings (1024 dimensions), full-text index
- **Why needed**:
  - Fast vector similarity search
  - BM25 keyword search
  - Hybrid search (combines both)
  - Optimized for high-dimensional vectors

**Example OpenSearch data**:
```json
{
  "paper_id": 42,
  "chunk_text": "We introduce VLPO, a reinforcement learning method...",
  "embedding": [0.123, 0.456, ..., 0.789],  // 1024 numbers
  "section_title": "Methods"
}
```

**How they work together**:
1. User query â†’ Convert to vector using Jina
2. OpenSearch finds similar vectors â†’ Returns paper IDs
3. PostgreSQL retrieves full metadata using paper IDs
4. Combined response sent to user

**This is industry standard**: ChatGPT, Notion AI, GitHub Copilot all use similar architecture (relational DB + vector DB).

---

### Q: What is the role of Jina? Is it a vector database?

**A**: **No, Jina is NOT a vector database**. This is a common confusion:

**Jina AI** = **Embeddings Service** (API)
- **What it does**: Converts text into vectors
- **Role**: Text-to-vector conversion
- **Example**:
  - Input: `"reinforcement learning"`
  - Output: `[0.123, 0.456, ..., 0.789]` (1024 numbers)

**OpenSearch** = **Vector Database** (Storage + Search)
- **What it does**: Stores vectors AND performs similarity search
- **Role**: Database that stores and searches vectors

**Analogy**:
- **Jina** = Camera (takes photos)
- **OpenSearch** = Google Photos (stores and searches photos)

**Complete Flow**:
```
Step 1: INDEXING
Paper text â†’ Jina API â†’ Vector â†’ OpenSearch (stores it)

Step 2: SEARCHING
User query â†’ Jina API â†’ Query vector â†’ OpenSearch (finds similar)
```

---

## ðŸ” Search & Retrieval

### Q: What is hybrid search and why is it better than just vector search?

**A**: Hybrid search combines two search methods:

**1. Vector Search (Semantic)**:
- Finds **meaning-based** similarity
- Uses embeddings (1024-dimensional vectors)
- Example: "ML" matches "machine learning" (different words, same meaning)

**2. BM25 Search (Keyword)**:
- Finds **exact keyword** matches
- Traditional full-text search
- Example: Finds exact term "reinforcement learning"

**Why combine both?**
- Vector search can miss exact technical terms
- Keyword search misses semantic similarity
- **Hybrid = Best of both worlds**

**Example**:
Query: "What papers discuss RL for vision?"

- Vector search finds: Papers about reinforcement learning + computer vision (semantic match)
- BM25 finds: Papers with exact acronym "RL" (keyword match)
- Hybrid combines scores for best results

**Implementation**: OpenSearch's RRF (Reciprocal Rank Fusion) pipeline combines both scores.

---

### Q: What is the "top_k" parameter in your search?

**A**: `top_k` controls how many relevant chunks are retrieved before generating an answer.

**Example**:
- `top_k = 1`: Retrieve 1 most relevant chunk â†’ Fast, less comprehensive
- `top_k = 3`: Retrieve 3 chunks â†’ Balanced (default)
- `top_k = 10`: Retrieve 10 chunks â†’ Detailed, slower, more expensive

**Trade-offs**:

| Value | Speed | Quality | Cost | Use Case |
|-------|-------|---------|------|----------|
| 1-2   | âš¡ Fast | Basic | ðŸ’° Cheap | Quick facts |
| 3-5   | âš¡ Good | âœ… Best | ðŸ’°ðŸ’° Moderate | Recommended |
| 6-10  | ðŸ¢ Slow | ðŸ“š Detailed | ðŸ’°ðŸ’°ðŸ’° Expensive | Deep research |

**Why it matters**: More chunks = more context for LLM = better answers, but higher API costs.

---

## ðŸ¤– RAG Pipeline

### Q: Walk me through the complete RAG pipeline when a user asks a question.

**A**: Here's the complete flow:

**Step 1: User Query**
```
User: "What papers discuss reinforcement learning?"
```

**Step 2: Query Embedding**
```
Jina API: Convert query to vector
â†’ [0.111, 0.222, ..., 0.888] (1024 dimensions)
```

**Step 3: Hybrid Search**
```
OpenSearch performs:
1. Vector similarity search (semantic)
2. BM25 keyword search
3. Combine with RRF â†’ Top 3 chunks
```

**Step 4: Retrieve Metadata**
```
PostgreSQL: Get paper metadata using paper_ids
â†’ Titles, authors, dates, abstracts
```

**Step 5: Context Construction**
```
Build context from retrieved chunks:
"Chunk 1: Monet paper discusses VLPO method..."
"Chunk 2: Another paper mentions RL for vision..."
"Chunk 3: Recent advances in policy optimization..."
```

**Step 6: LLM Generation**
```
Send to OpenAI GPT-4o-mini:
Context: [3 chunks]
Question: "What papers discuss reinforcement learning?"
â†’ Generate comprehensive answer with citations
```

**Step 7: Response**
```
Return to user:
- AI-generated answer
- Source paper links
- Metadata (chunks used, search mode)
```

**Total time**: ~2-5 seconds

---

## ðŸ“Š Data & Indexing

### Q: Why do you only index title + abstract instead of full paper content?

**A**: Currently, we index only title + abstract for **100 papers** as a proof of concept. This means:

**Current State**:
- 100 papers Ã— 1 chunk each = 100 chunks total
- Each chunk: Title + Abstract (~200 words)
- **Limitation**: Can't answer detailed methodology questions

**Next Iteration** (planned):
- 50 papers with **full PDF content**
- ~20-40 chunks per paper
- 50 papers Ã— 30 chunks = **~1500 chunks**
- Includes: Introduction, Methods, Experiments, Results, Conclusion

**Why full content is better**:

**Question**: "How does Monet's VLPO method work?"

**Current answer** (abstract only):
> "The paper introduces VLPO, a reinforcement learning method..."

**Future answer** (full content):
> "VLPO works by explicitly incorporating latent embeddings into policy gradient updates. The method uses a three-stage pipeline: First, distillation-based SFT... Second, policy optimization using... Experiments show 15% improvement..."

**Much more specific and detailed!**

---

### Q: What is text chunking and why is it important?

**A**: Text chunking splits long documents into smaller, semantically meaningful pieces.

**Why chunk?**
1. **LLM context limits**: Can't send entire 20-page paper to LLM
2. **Better retrieval**: Specific sections match queries better than whole documents
3. **Cost optimization**: Only send relevant chunks, not entire papers

**Our chunking strategy**:
- **Chunk size**: 600 words (configurable)
- **Overlap**: 100 words (prevents splitting important info)
- **Section-based**: Respects paper structure (Introduction, Methods, etc.)

**Example**:
```
Paper (10,000 words)
â†“
Chunking
â†“
- Chunk 0: Abstract (200 words)
- Chunk 1: Introduction (600 words)
- Chunk 2: Introduction continued (600 words, 100 overlap)
- Chunk 3: Methods (600 words)
- ...
- Chunk 20: Conclusion (400 words)
```

**Result**: 20 chunks that can be independently searched and retrieved.

---

## ðŸš€ Deployment & Infrastructure

### Q: How is your system deployed?

**A**: Deployed on **Railway.app** with the following services:

**Services Deployed**:
1. **FastAPI Backend** (Port 8000)
   - Handles API requests
   - Orchestrates all services

2. **PostgreSQL** (Managed Database)
   - Stores paper metadata
   - 100 papers currently

3. **OpenSearch** (Search Engine)
   - Vector + keyword search
   - 100 chunks indexed

4. **Redis** (Cache)
   - Caches API responses
   - Improves performance

**Frontend**:
- **Streamlit** (can deploy to Streamlit Cloud for free)
- Currently runs locally, can be deployed separately

**Configuration**:
- Environment variables via Railway
- Secrets managed securely
- Public URLs for all services

**Cost**: ~$5-10/month (Railway Hobby plan)

---

### Q: What challenges did you face during deployment?

**A**: Several interesting challenges:

**1. Docker Build Issues**:
- **Problem**: Railway didn't support certain Docker cache directives
- **Solution**: Simplified Dockerfile, removed mount directives

**2. Environment Variable Loading**:
- **Problem**: Scripts not loading `.env` file properly
- **Solution**: Created wrapper scripts that explicitly source environment variables

**3. Database Connection**:
- **Problem**: Services trying to connect to localhost instead of Railway URLs
- **Solution**: Proper environment variable configuration with Pydantic Settings

**4. Data Import**:
- **Problem**: Needed to import 100 papers from local to Railway PostgreSQL
- **Solution**: Used Docker PostgreSQL client to pipe SQL dump to remote database

**5. OpenSearch Indexing**:
- **Problem**: 0 documents in OpenSearch after deployment
- **Solution**: Created reindexing script to populate OpenSearch from PostgreSQL

---

## ðŸ’» Technical Implementation

### Q: What technologies and frameworks did you use?

**A**:

**Backend**:
- **FastAPI**: REST API framework (Python)
- **SQLAlchemy**: ORM for PostgreSQL
- **Pydantic**: Data validation and settings management
- **opensearch-py**: OpenSearch client

**Frontend**:
- **Streamlit**: Web UI framework (Python)
- **requests**: HTTP client for API calls

**AI/ML**:
- **Jina AI**: Embeddings API (jina-embeddings-v3, 1024 dimensions)
- **OpenAI**: GPT-4o-mini for answer generation

**Infrastructure**:
- **Railway**: Cloud deployment platform
- **Docker**: Containerization
- **uv**: Python package manager (faster than pip)

**Development Tools**:
- **Git/GitHub**: Version control
- **pytest**: Testing framework
- **Claude Code**: AI-assisted development

---

## ðŸŽ¯ Project Outcomes

### Q: What were the key achievements of this project?

**A**:

**1. Fully Functional RAG System**:
- âœ… 100 papers indexed and searchable
- âœ… Hybrid search (semantic + keyword)
- âœ… AI-powered question answering with citations
- âœ… Production deployment on Railway

**2. Complete Pipeline**:
- âœ… Paper fetching from arXiv API
- âœ… PDF parsing and text extraction
- âœ… Smart chunking and indexing
- âœ… Embedding generation
- âœ… Vector search implementation

**3. User Interface**:
- âœ… Clean Streamlit frontend
- âœ… Real-time API health monitoring
- âœ… Configurable search parameters
- âœ… Source attribution

**4. Documentation**:
- âœ… Comprehensive deployment guides
- âœ… Architecture documentation
- âœ… Setup instructions
- âœ… Troubleshooting guides

**5. Scalability**:
- âœ… Designed to handle thousands of papers
- âœ… Modular architecture
- âœ… Easy to add new features
- âœ… Cloud-native deployment

---

## ðŸ”® Future Enhancements

### Q: What improvements would you make next?

**A**:

**Short-term (Next Sprint)**:
1. **Full Content Indexing**:
   - Index 50 papers with complete PDF content
   - ~1500 chunks for better answer quality

2. **Enhanced UI**:
   - Paper browsing/filtering
   - Search history
   - Export functionality

**Medium-term**:
3. **Automated Ingestion**:
   - Deploy Airflow for daily paper updates
   - Automated PDF processing pipeline

4. **Advanced Features**:
   - Citation network visualization
   - Paper recommendation system
   - Custom collections

**Long-term**:
5. **Production Features**:
   - User authentication
   - Multi-tenancy
   - Analytics dashboard
   - API rate limiting

6. **Performance**:
   - Streaming responses
   - Response caching
   - Query optimization

---

## ðŸŽ¤ Interview Tips

### Q: How would you explain this project to a non-technical person?

**A**:

"I built an AI-powered research assistant for scientific papers. Think of it like asking ChatGPT about research papers, but it only uses actual scientific papers as sources and always cites them.

For example, if you ask 'What research has been done on reinforcement learning?', it searches through 100 AI research papers, finds the most relevant sections, and generates a comprehensive answer with links to the original papers.

It's like having a research librarian who's read all 100 papers and can instantly find and summarize the relevant information for you."

### Q: What was the most challenging part?

**A**:

"The most challenging part was getting the environment variable configuration right for the Railway deployment. The issue was that Pydantic Settings has a specific way of handling nested configuration with prefixes like `OPENSEARCH__HOST`, and the script wasn't loading the `.env` file properly.

I had to debug by:
1. Adding logging to see what values were being loaded
2. Understanding Pydantic's configuration hierarchy
3. Creating wrapper scripts to explicitly load environment variables
4. Testing the connection to Railway services

This taught me a lot about configuration management in production environments and the importance of proper environment variable handling."

### Q: Why is this project valuable?

**A**:

"This project demonstrates several valuable skills:

1. **Full-stack AI development**: Frontend, backend, databases, AI integration
2. **Production deployment**: Not just local development, actually deployed and accessible
3. **Modern tech stack**: FastAPI, vector databases, LLMs, cloud deployment
4. **Problem-solving**: Overcame deployment challenges, configuration issues
5. **Real-world application**: Solves actual research discovery problems

It's also a complete RAG system, which is one of the hottest topics in AI right now. Companies are actively looking for people who understand how to build production RAG systems."

---

## ðŸ“ˆ Metrics & Performance

**Current System Stats**:
- **Papers indexed**: 100
- **Chunks**: 100 (title + abstract only)
- **Vector dimensions**: 1024
- **Average query time**: 2-5 seconds
- **Search mode**: Hybrid (BM25 + Vector)
- **Deployment**: Railway.app
- **Uptime**: 99%+

**API Endpoints**:
- `/api/v1/ask` - Question answering
- `/api/v1/health` - Health check
- `/api/v1/hybrid-search/` - Direct search
- `/api/v1/stream` - Streaming responses

**Frontend**:
- Streamlit running on port 8501
- Real-time API monitoring
- Configurable search parameters

---

## ðŸ”— Resources

- **GitHub**: https://github.com/sudhirshivaram/arxiv-paper-curator-v1
- **API Docs**: https://arxiv-paper-curator-v1-production.up.railway.app/docs
- **Deployment**: Railway.app

---

**Last Updated**: December 2024
**Project Duration**: 5 weeks (part-time)
**Technologies**: 15+ (Python, FastAPI, Streamlit, PostgreSQL, OpenSearch, Docker, Railway, etc.)
